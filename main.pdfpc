{"pdfpcFormat":2,"disableMarkdown":false,"pages":[{"idx":0,"label":1,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":1,"label":2,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- In recent years, Memory Safety of Applications and Programming Languages has received increasing interest. Due to an increasing dependence of our privacy on the security of digital systems, memory safety as one piece of secure systems, is becoming more and more important.\n  - However, experience hash shown time and time again, that high-impact vulnerabilities due to memory unsafety are virtually unavoidable in large projects written in C/C++.\n  - A recent examination of the high severity security impacting bugs in Chromium revealed that 70 % are due to memory unsafety. And, this is corroborated by other large projects, such as Windows and Android.\n  - This is relevant, as MPC applications are networked services, potentially exposed to the Internet, and which are usually written in C/C++. \n  - The Memory Safety of these applications will becomer more important as MPC progesses from research to real-world deployments.\n  - The reason C/C++ are so often used in MPC, is the performance and efficiency of the resulting implementations.\n  [NEXT SLIDE]\n  - To shift MPC from the largely theoretical to the practical we need to optimize its performance and efficiency\n  - One aspect of efficiency which we've focused on with SEEC, is memory efficiency\n  - This is especially relevant if we want to target mobile devices, where the most common RAM configuration was 4 GB in 2022.\n  - But it's also relevant to deployments on servers, where reduced memory consumption can allow us to tackle larger problems or reduce operating costs"},{"idx":2,"label":2,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- In recent years, Memory Safety of Applications and Programming Languages has received increasing interest. Due to an increasing dependence of our privacy on the security of digital systems, memory safety as one piece of secure systems, is becoming more and more important.\n  - However, experience hash shown time and time again, that high-impact vulnerabilities due to memory unsafety are virtually unavoidable in large projects written in C/C++.\n  - A recent examination of the high severity security impacting bugs in Chromium revealed that 70 % are due to memory unsafety. And, this is corroborated by other large projects, such as Windows and Android.\n  - This is relevant, as MPC applications are networked services, potentially exposed to the Internet, and which are usually written in C/C++. \n  - The Memory Safety of these applications will becomer more important as MPC progesses from research to real-world deployments.\n  - The reason C/C++ are so often used in MPC, is the performance and efficiency of the resulting implementations.\n  [NEXT SLIDE]\n  - To shift MPC from the largely theoretical to the practical we need to optimize its performance and efficiency\n  - One aspect of efficiency which we've focused on with SEEC, is memory efficiency\n  - This is especially relevant if we want to target mobile devices, where the most common RAM configuration was 4 GB in 2022.\n  - But it's also relevant to deployments on servers, where reduced memory consumption can allow us to tackle larger problems or reduce operating costs"},{"idx":3,"label":3,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay, so what do we contribute with SEEC?\n- Because we wanted to achieve a memory safe and efficient MPC framework, which also provides good performance and a nice developer experience, the natural choice was Rust as a programming language for us.\n- It's a memory safe language, with performance similar to C/C++, control over memory allocations without garbage collection, and a good developer experience due to fantastic tooling.\n- With SEEC, we provide a high-level embedded domain specific language to contruct circuits, and also the option to use FUSE or Bristol circuits.\n- We provide a memory- and round-efficient implementation of sub-circuits with optional support for MPC level SIMD.\n- And our API supports both function-independent and dependent preprocessing.\n- What I personally really like, is that SEEC is extensibile without a need for forking if you use it as a library to implement your protocol in.\n- And, it is also cross-platform which we test in our continous integration pipeline."},{"idx":4,"label":4,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Currently, SEEC is aimed at linear secret sharing based protocols, think GMW, with semi-honest security and two parties. \n- We plan to extend SEEC to also provide facilities for multiple-parties and maliciously secure protocols.\n- The currently implemented protocols are GMW with multiplication triples in the boolean and arithmetic domain, and also a mixed domain implementation\n- We also provide a partial implementation of the ASTRA and ABY2.0 protocols for the Boolean domain.\n- Notably, we have also implemented an OT library which uses Chou Orlandis Simples OT, and the ALSZ13 and  Silent OT extension protocols\n- As part of this work, we also developed a benchmarking tool which supports ABY, MOTION, MP-SPDZ, and SEEC.\n- You can use it to declaratively specify benchmarks with varying input sizes\n- And then it handles the compilation, local or remote execution of parties, and parsing of the results into a common format."},{"idx":5,"label":5,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay now on to some of the implementation details of SEEC, specifically those related to memory efficiency.\n- Here, on the left side we have some simple Rust code with a function that takes two Booleans and returns a Boolean, which is called twice where the result of the first call is one input for the second call.\n- In traditional programs, we use functions as an important tool for building abstractions and organizing code\n- they also reduce the size of the binary, \n- Imagine if we would inline every function of a large program. The result would be an enormous binary.\n  - and yet, this is exactly what we often do in secret-sharing based MPC, resulting in huge circuits"},{"idx":6,"label":6,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay, so now on the right side we see what we want secure programs to roughly look like\n- Crucially, we want to have a high-level way of specifying a functionality to evaluate securely\n- With our framework SEEC, we've tried to tackle the problem of memory and communication round efficient functions or in our case sub-circuits.\n- Ideally, we can declare functions in our high-level functionality and use them as normal functions within our secure functionality to ease the development of MPC applications.\n- some slight differences such as the changed types here, are okay; and likely necessary\n- with our work SEEC, we have implemented and extensively benchmarked and compared one possible solution"},{"idx":7,"label":7,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay, so what is the challenge for functions in MPC, specifically for linear secret-sharing based protocols which execute in multiple rounds?\n- Let's say we have our high-level functionality with two function calls.\n- In this case, these two function calls are independent of each other\n[NEXT SLIDE]\n- If we go a level lower and look at intermediate or in-memory representation which the source code of our secure functionality is compiled to, we have two options:\n- We can represent the functionality as Bytecode for an MPC-specific virtual machine, for example this is how MP-SPDZ does it.\n- Or, we can represent the functionality as a circuit or directed acyclic graph, which more closely resembles descriptions of protocols in the literature and is the approach taken by ABY, MOTION and others.\n- In the bytecode representation, we have the bytecode for the function once in memory and then two sequential call instructions to this function.\n- Whereas in the graph based approach, we repeat the sub-graph corresponding to the function within the larger circuit for our two calls.\n[NEXT SLIDE]\n- Both approaches have some advantages and disadvantages.\n- With the bytecode executed by VM, the sequential function calls result in an increased number of communication rounds.\n- However, the definition of the function itself is only stored once in memory and we can more easily reuse memory for the output of gates or instructions in this case.\n- If we choose a graph approach, we naturally get concurrent evaluation of these two functions when we iterate over the layers of the circuit.\n- However, this comes at the cost of increased memory, as the sub-circuit for the function is repeated in memory.\n\n- What we've looked at is: Can we have memory efficient functions or sub-circuits which do not increase the number of rounds?\n- As a side note: With garbled circuits, functions are much easier to support, as there, the sequential evaluation does not matter, because of the constant round evaluation."},{"idx":8,"label":7,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- Okay, so what is the challenge for functions in MPC, specifically for linear secret-sharing based protocols which execute in multiple rounds?\n- Let's say we have our high-level functionality with two function calls.\n- In this case, these two function calls are independent of each other\n[NEXT SLIDE]\n- If we go a level lower and look at intermediate or in-memory representation which the source code of our secure functionality is compiled to, we have two options:\n- We can represent the functionality as Bytecode for an MPC-specific virtual machine, for example this is how MP-SPDZ does it.\n- Or, we can represent the functionality as a circuit or directed acyclic graph, which more closely resembles descriptions of protocols in the literature and is the approach taken by ABY, MOTION and others.\n- In the bytecode representation, we have the bytecode for the function once in memory and then two sequential call instructions to this function.\n- Whereas in the graph based approach, we repeat the sub-graph corresponding to the function within the larger circuit for our two calls.\n[NEXT SLIDE]\n- Both approaches have some advantages and disadvantages.\n- With the bytecode executed by VM, the sequential function calls result in an increased number of communication rounds.\n- However, the definition of the function itself is only stored once in memory and we can more easily reuse memory for the output of gates or instructions in this case.\n- If we choose a graph approach, we naturally get concurrent evaluation of these two functions when we iterate over the layers of the circuit.\n- However, this comes at the cost of increased memory, as the sub-circuit for the function is repeated in memory.\n\n- What we've looked at is: Can we have memory efficient functions or sub-circuits which do not increase the number of rounds?\n- As a side note: With garbled circuits, functions are much easier to support, as there, the sequential evaluation does not matter, because of the constant round evaluation."},{"idx":9,"label":7,"overlay":2,"forcedOverlay":true,"hidden":false,"note":"- Okay, so what is the challenge for functions in MPC, specifically for linear secret-sharing based protocols which execute in multiple rounds?\n- Let's say we have our high-level functionality with two function calls.\n- In this case, these two function calls are independent of each other\n[NEXT SLIDE]\n- If we go a level lower and look at intermediate or in-memory representation which the source code of our secure functionality is compiled to, we have two options:\n- We can represent the functionality as Bytecode for an MPC-specific virtual machine, for example this is how MP-SPDZ does it.\n- Or, we can represent the functionality as a circuit or directed acyclic graph, which more closely resembles descriptions of protocols in the literature and is the approach taken by ABY, MOTION and others.\n- In the bytecode representation, we have the bytecode for the function once in memory and then two sequential call instructions to this function.\n- Whereas in the graph based approach, we repeat the sub-graph corresponding to the function within the larger circuit for our two calls.\n[NEXT SLIDE]\n- Both approaches have some advantages and disadvantages.\n- With the bytecode executed by VM, the sequential function calls result in an increased number of communication rounds.\n- However, the definition of the function itself is only stored once in memory and we can more easily reuse memory for the output of gates or instructions in this case.\n- If we choose a graph approach, we naturally get concurrent evaluation of these two functions when we iterate over the layers of the circuit.\n- However, this comes at the cost of increased memory, as the sub-circuit for the function is repeated in memory.\n\n- What we've looked at is: Can we have memory efficient functions or sub-circuits which do not increase the number of rounds?\n- As a side note: With garbled circuits, functions are much easier to support, as there, the sequential evaluation does not matter, because of the constant round evaluation."},{"idx":10,"label":8,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- With SEEC, we provide a memory and round-efficient way of using sub-circuits in secret-sharing based protocols.\n- With our high-level API, you can annotate a normal Rust function, which operates on data of type Secret, with our sub_circuit macro\n- In this case, the function simply computes the pairwise AND of two vectors\n[NEXT SLIDE]\n- Then we can use this function as a normal function in the rest of our code.\n- Okay, so for an MPC application developer this is a nice experience, but how does it work?"},{"idx":11,"label":8,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- With SEEC, we provide a memory and round-efficient way of using sub-circuits in secret-sharing based protocols.\n- With our high-level API, you can annotate a normal Rust function, which operates on data of type Secret, with our sub_circuit macro\n- In this case, the function simply computes the pairwise AND of two vectors\n[NEXT SLIDE]\n- Then we can use this function as a normal function in the rest of our code.\n- Okay, so for an MPC application developer this is a nice experience, but how does it work?"},{"idx":12,"label":9,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- In SEEC, we also use a graph based representation of the functionality to execute.\n- However, in contrast to previous works, repeated calls to a functionality will not inline them for each call.\n- Instead, for the first call to a sub-circuit, we build the sub-graph corresponding to this function and cache it.\n- Subsequent calls to the function will not recompute the sub-circuit, but instead connect the argument gates of the calling circuit to the input gates of the called sub-circuit.\n- For these connections between called circuits, we optimized the memory required to store them, specifically we compress gates with consecutive Ids into ranges of gates.\n- Okay, so the first part is done, we have a memory efficient way of declaring sub-circuits, but what about the round complexity?\n[NEXT SLIDE]\n- For an efficient online phase, it is important that using sub-circuits does not increase the number of communication rounds.\n- In our framework, we achieve this via lazy iteration over the topologically sorted layers for each **use** of a subcircuit.\n- We refer to this lazy iteration of sub-circuit layers as dynamic layers or DL for short.\n- The crucial property of this approach, is that the execution of each sub-circuit is **as if it was inlined**, meaning we have no increase in the depth or round complexity of the overall circuit.\n- Also, this graph-based lazy iteration approach enables us to have partial and concurrent evaluation of multiple uses of the same sub-circuit or different sub-circuits.\n- This notion and our implementation of sub-circuits enables us to have memory- and round-efficient functions for secret-sharing based MPC protocols."},{"idx":13,"label":9,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- In SEEC, we also use a graph based representation of the functionality to execute.\n- However, in contrast to previous works, repeated calls to a functionality will not inline them for each call.\n- Instead, for the first call to a sub-circuit, we build the sub-graph corresponding to this function and cache it.\n- Subsequent calls to the function will not recompute the sub-circuit, but instead connect the argument gates of the calling circuit to the input gates of the called sub-circuit.\n- For these connections between called circuits, we optimized the memory required to store them, specifically we compress gates with consecutive Ids into ranges of gates.\n- Okay, so the first part is done, we have a memory efficient way of declaring sub-circuits, but what about the round complexity?\n[NEXT SLIDE]\n- For an efficient online phase, it is important that using sub-circuits does not increase the number of communication rounds.\n- In our framework, we achieve this via lazy iteration over the topologically sorted layers for each **use** of a subcircuit.\n- We refer to this lazy iteration of sub-circuit layers as dynamic layers or DL for short.\n- The crucial property of this approach, is that the execution of each sub-circuit is **as if it was inlined**, meaning we have no increase in the depth or round complexity of the overall circuit.\n- Also, this graph-based lazy iteration approach enables us to have partial and concurrent evaluation of multiple uses of the same sub-circuit or different sub-circuits.\n- This notion and our implementation of sub-circuits enables us to have memory- and round-efficient functions for secret-sharing based MPC protocols."},{"idx":14,"label":10,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay, so we just saw that with SEEC we can have efficient sub-circuits.\n- But there also a different technique, namely SIMD, which stands for single instruction, multiple data, and has been used to reduce memory consumption and increase efficiency of MPC implementations.\n- For example in the MOTION framework, we can have SIMD gates, which perform the same operation on vectors of data instead of individual values.\n  - A nice benefit of these MPC SIMD gates, is that these gates internally can use hardware SIMD instructions to speed up the computation\n[NEXT SLIDE]\n- In SEEC, we also support SIMD, but instead of at the gate level, we support SIMD sub-circuits.\n- So instead of an individual gate that operates on vectors, we have sub-circuits that operate on vectors of data.\n- We do this because supporting SIMD at the gate level can very easily increase the memory consumption of circuits which **don't use SIMD**."},{"idx":15,"label":10,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- Okay, so we just saw that with SEEC we can have efficient sub-circuits.\n- But there also a different technique, namely SIMD, which stands for single instruction, multiple data, and has been used to reduce memory consumption and increase efficiency of MPC implementations.\n- For example in the MOTION framework, we can have SIMD gates, which perform the same operation on vectors of data instead of individual values.\n  - A nice benefit of these MPC SIMD gates, is that these gates internally can use hardware SIMD instructions to speed up the computation\n[NEXT SLIDE]\n- In SEEC, we also support SIMD, but instead of at the gate level, we support SIMD sub-circuits.\n- So instead of an individual gate that operates on vectors, we have sub-circuits that operate on vectors of data.\n- We do this because supporting SIMD at the gate level can very easily increase the memory consumption of circuits which **don't use SIMD**."},{"idx":16,"label":11,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Alongside sub-circuits and SIMD, we also implement several optimizations to increase performance and memory efficiency\n- The first one of those, we call static layers\n- Layers here refers to the layers of **uses of a sub-circuit** which are dynamically computed in the dynamic layer representation\n- With the static layer representation, we precompute those layers of each use of a sub-circuit\n- To not blow up the required memory, we however only store unique layer configurations of a sub-circuit.\n- As a result, if two calls to sub-circuit result in the configuration of sub-circuit layers, we only store these once.\n- The benefit of this optimization is twofold:\n  - increased perfomance of the online phase, because we don't need to dynamically compute the layers on the fly\n  - And, depending on the circuit topology, a reduced memory consumption because we don't need a general graph representation with forward and backward references between gates anymore.\n[NEXT SLIDE]\n- The next optimization is early deallocation of gate outputs in SIMD circuits\n- Because in SIMD circuits, the outputs of gates are heap allocated vectors,\nwe can free them once they're not needed for further evaluation. \n[NEXT SLIDE]\n- And lastly, with our generic APIs for preprocessing, we have the option to precompute multiplication triples and store them on the filesystem.\n- In the online phase, we can read the triples on-demand in batches from a file.\n- This reduces memory consumption, as we don't need hold all required triples in memory at once."},{"idx":17,"label":11,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- Alongside sub-circuits and SIMD, we also implement several optimizations to increase performance and memory efficiency\n- The first one of those, we call static layers\n- Layers here refers to the layers of **uses of a sub-circuit** which are dynamically computed in the dynamic layer representation\n- With the static layer representation, we precompute those layers of each use of a sub-circuit\n- To not blow up the required memory, we however only store unique layer configurations of a sub-circuit.\n- As a result, if two calls to sub-circuit result in the configuration of sub-circuit layers, we only store these once.\n- The benefit of this optimization is twofold:\n  - increased perfomance of the online phase, because we don't need to dynamically compute the layers on the fly\n  - And, depending on the circuit topology, a reduced memory consumption because we don't need a general graph representation with forward and backward references between gates anymore.\n[NEXT SLIDE]\n- The next optimization is early deallocation of gate outputs in SIMD circuits\n- Because in SIMD circuits, the outputs of gates are heap allocated vectors,\nwe can free them once they're not needed for further evaluation. \n[NEXT SLIDE]\n- And lastly, with our generic APIs for preprocessing, we have the option to precompute multiplication triples and store them on the filesystem.\n- In the online phase, we can read the triples on-demand in batches from a file.\n- This reduces memory consumption, as we don't need hold all required triples in memory at once."},{"idx":18,"label":11,"overlay":2,"forcedOverlay":true,"hidden":false,"note":"- Alongside sub-circuits and SIMD, we also implement several optimizations to increase performance and memory efficiency\n- The first one of those, we call static layers\n- Layers here refers to the layers of **uses of a sub-circuit** which are dynamically computed in the dynamic layer representation\n- With the static layer representation, we precompute those layers of each use of a sub-circuit\n- To not blow up the required memory, we however only store unique layer configurations of a sub-circuit.\n- As a result, if two calls to sub-circuit result in the configuration of sub-circuit layers, we only store these once.\n- The benefit of this optimization is twofold:\n  - increased perfomance of the online phase, because we don't need to dynamically compute the layers on the fly\n  - And, depending on the circuit topology, a reduced memory consumption because we don't need a general graph representation with forward and backward references between gates anymore.\n[NEXT SLIDE]\n- The next optimization is early deallocation of gate outputs in SIMD circuits\n- Because in SIMD circuits, the outputs of gates are heap allocated vectors,\nwe can free them once they're not needed for further evaluation. \n[NEXT SLIDE]\n- And lastly, with our generic APIs for preprocessing, we have the option to precompute multiplication triples and store them on the filesystem.\n- In the online phase, we can read the triples on-demand in batches from a file.\n- This reduces memory consumption, as we don't need hold all required triples in memory at once."},{"idx":19,"label":12,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Okay, so what impact do sub-circuits and our optimizations have on the memory efficiency?\n- To evaluate this, we have run extensive benchmarks with our dedicated benchmarking tool which I mentioned in the beginning.\n- We compare with the ABY, MP-SPDZ, and MOTION frameworks\n- In all frameworks, we use the semi-honest two-party Boolean GMW protocol with multiplication triples and only benchmark the online phase and not the setup.\n- We perform the evaluation in different environments, with differing simluated network settings and a varying number of threads.\n- To measure the maximum memory consumption, we use the heaptrack tool.\n- We ran each party on one of our servers with 32 logical cores and 128 Gigs of RAM.\n[NEXT SLIDE]\n- We benchmark two kinds of functionalities.\n- In the first one, we repeatedly execute AES-128 chained after one another.\n  - So this is essentially AES in CBC mode in MPC.\n  - We chose this functionality as it has this structure of a sub-functionality which is repeatedly used, it can not exploit SIMD, the size of the circuit can easily be scaled, and it was straightforward to implement in the different frameworks.\n- And then also, we benchmark SIMD evaluations of AES and SHA-256 with different SIMD vector sizes to see the impact of our SIMD specific optimizations."},{"idx":20,"label":12,"overlay":1,"forcedOverlay":true,"hidden":false,"note":"- Okay, so what impact do sub-circuits and our optimizations have on the memory efficiency?\n- To evaluate this, we have run extensive benchmarks with our dedicated benchmarking tool which I mentioned in the beginning.\n- We compare with the ABY, MP-SPDZ, and MOTION frameworks\n- In all frameworks, we use the semi-honest two-party Boolean GMW protocol with multiplication triples and only benchmark the online phase and not the setup.\n- We perform the evaluation in different environments, with differing simluated network settings and a varying number of threads.\n- To measure the maximum memory consumption, we use the heaptrack tool.\n- We ran each party on one of our servers with 32 logical cores and 128 Gigs of RAM.\n[NEXT SLIDE]\n- We benchmark two kinds of functionalities.\n- In the first one, we repeatedly execute AES-128 chained after one another.\n  - So this is essentially AES in CBC mode in MPC.\n  - We chose this functionality as it has this structure of a sub-functionality which is repeatedly used, it can not exploit SIMD, the size of the circuit can easily be scaled, and it was straightforward to implement in the different frameworks.\n- And then also, we benchmark SIMD evaluations of AES and SHA-256 with different SIMD vector sizes to see the impact of our SIMD specific optimizations."},{"idx":21,"label":13,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- First, we examine the peak memory of SEEC on the AES-CBC circuit.\n- Note that both the x and y axis are logarithmically scaled.\n- SEEC with dynamic layers but without sub-circuits, so each AES circuit is inlined, has the highest peak memory consumption with 50 GB at the largest circuit size.\n- If we then use the dynamic layer representation with sub-circuits, the memory consumption is reduced to 2.2 GB.\n- And if we further use the static layer optimization, we can further reduce this to just 375 MB. So this is roughly a factor 135 improvement when comparing no sub-circuits to sub-circuits with static layers.\n- But how does this stack up against the other frameworks?"},{"idx":22,"label":14,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- When comparing with the other frameworks, we see that especially MOTION has tremendous memory overhead, especially for Boolean circuits not using SIMD.\n- Only at 100 chained AES circuits, the peak memory consumption of the online phase is already 73 GB and we could not evaluate larger sizes as we rant out of RAM.\n- ABY fares better with 74 GB at 10K chained circuits, but this is still more than the 50 GB of SEEC without sub-circuits.\n- For MP-SPDZ and SEEC, the difference is quite small. While SEEC fares better for very small circuits, with increasing size the memory consumption of MP-SPDZ is better than SEEC.\n- One reason for MP-SPDZ low memory consumption in this benchmark is its support of loops, which for this circuit don't increase the number of rounds, but significantly reduce the size of the bytecode.\n- So we confirmed our earlier claims that the bytecode representation of MP-SPDZ results in low memory consumption for circuits such as the evaluated, whereas graph-based approaches tend to require several orders of magnitude more memory.\n- We also see that, with our implementation of sub-circuits we achieve memory efficiency that is close to the bytecode approach."},{"idx":23,"label":15,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Alright, so now let us focus on SIMD circuitss, where we operate on vectors of values instead of individual ones.\n- So here, we evaluate the AES-128 circuit with varying SIMD sizes in parallel and again look at the peak memory consumption.\n- For SEEC, we have the highest peak memory consumption of 7.5 GB at 1 million parallel AES calls for the variant where we only use static layers but no early deallocation or triples streaming.\n- This is already improved by using early deallocation of SIMD gate outputs and further improved by triples streaming to just 700 MB.\n- However, if we look at left side of the graph, we see that the variant with streaming triples requires the most memory.\n- The reason for this is, that we read the triples in a pre-defined batch size.\n- This batch size was quite high for this benchmark, so the majority of the memory for small circuits is due to unneeded triples being loaded into memory.\n- So we get more than a factor of 10 reduction in peak memory for this high SIMD use case when using our optimizations and SEEC.\n- So now, lets again compare the most efficient variant with the other frameworks."},{"idx":24,"label":16,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- For ABY and MP-SPDZ, we were unable to go up to the largest SIMD size as these frameworks crashed for this size.\n- MOTION managed to evaluate 1 million parallel calls, but required 10 GB of peak memory for this, compared to SEECs 700 MB we saw earlier.\n- And again, on the left side of the graph, SEEC performs worse for smaller circuits due to the triple batch size."},{"idx":25,"label":17,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- Now, on to the last benchmark, where we look at the runtime of the AES-CBC benchmark with 100 chained circuits in our three network settings.\n- In general, ABY performs really well runtime wise for this circuit and tends to be a little bit faster than SEEC, except in the WAN setting.\n- Interestingly, MP-SPDZ has a very good runtime in the lowest latency network setting\n- But then, the runtime curiously increases over-proportional with the latency, with MP-SPDZ being the slowest in the WAN setting with 616 seconds, twice as much as SEEC.\n- For MOTION, we see that it can't really make use of low-latency networks and has the highest runtime in the LAN settings.\n- In general, SEEC has good online runtime performance in all network settings, with room for improvement in the low latency network.\n- Most importantly, using sub-circuits does not increase the number of communication rounds!"},{"idx":26,"label":18,"overlay":0,"forcedOverlay":false,"hidden":false,"note":"- With SEEC, we achieve a memory-safe and efficient two party secure computation framework\n- We provide a memory and communication round efficient implementation of sub-circuits with enable the use of functions in the functionalities for secret-sharing bases MPC\n- support for loops and register allocation of gate outputs can lead to better memory efficiency in some cases (MP-SPDZ)\n- however sub-circuits are more versatile, as they can reduce memory consumption of sub-circuit calls at unrelated places of the main circuit\nSIMD\n- Significantly better SIMD memory consumption\n  - largely due to FG and IS optimizations\n- Predicatbility\n- Realiability"},{"idx":27,"label":19,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":28,"label":20,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":29,"label":21,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":30,"label":22,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":31,"label":23,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":32,"label":24,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":33,"label":25,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":34,"label":26,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":35,"label":27,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":36,"label":28,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":37,"label":29,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":38,"label":30,"overlay":0,"forcedOverlay":false,"hidden":false},{"idx":39,"label":31,"overlay":0,"forcedOverlay":false,"hidden":false}]}